{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940289d6-ceaa-4d2c-9158-1b7ee8dbe69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/manitou/pmg/projects/bys2107/cp-fnr'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a340407-e2db-47f3-9535-0940b452194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 12 16:21:41 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000                On | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   34C    P8               24W / 300W|      0MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb66be37-d219-4f88-a634-1fc907b6f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a118db-44cc-40e2-8e3f-7604eafde962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "## evaluation module\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c82c997-d303-476f-94af-eb41d249fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.ensemble import RakelD\n",
    "from skmultilearn.ensemble import RakelO\n",
    "from skmultilearn.ensemble import MajorityVotingClassifier\n",
    "from skmultilearn.cluster import FixedLabelSpaceClusterer\n",
    "from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from skmultilearn.cluster import MatrixLabelSpaceClusterer\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191747db-b870-464a-9717-ed9846b15a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mdata\u001b[0m/                     \u001b[01;32mmain.py\u001b[0m*                \u001b[34;42m__pycache__\u001b[0m/       \u001b[01;32mtrain.py\u001b[0m*\n",
      "\u001b[01;32mexperiments-ecoli.ipynb\u001b[0m*  model-baseline-mlp.pth  \u001b[01;32mREADME.txt\u001b[0m*        \u001b[01;32mutils.py\u001b[0m*\n",
      "\u001b[01;32mfun.py\u001b[0m*                   \u001b[34;42mmodels\u001b[0m/                 \u001b[01;32mrequirements.txt\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2547c1-e2ec-4763-9a76-337128e49b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_data = pd.read_csv(\"data/cip_ctx_ctz_gen_multi_data.csv\",index_col=0)\n",
    "gi_pheno = pd.read_csv(\"data/cip_ctx_ctz_gen_pheno.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2c6adb-3af4-4ae2-8d1e-4da23954341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (566, 60936)\n",
      "Y_train shape: (566, 4)\n",
      "X_calibration shape: (121, 60936)\n",
      "Y_calibration shape: (121, 4)\n",
      "X_test shape: (122, 60936)\n",
      "Y_test shape: (122, 4)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 1000\n",
    "\n",
    "def constructTrainCalTestSplit(X, Y, train_ratio=0.7, calibration_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + calibration_ratio + test_ratio == 1\n",
    "    # Create a list of indices to shuffle\n",
    "    indices = list(X.index)\n",
    "    random.Random(SEED).shuffle(indices)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_split_point = int(len(indices) * train_ratio)\n",
    "    calibration_split_point = train_split_point + int(len(indices) * calibration_ratio)\n",
    "    \n",
    "    # Split indices into training, calibration, and testing sets\n",
    "    train_indices = indices[:train_split_point]\n",
    "    calibration_indices = indices[train_split_point:calibration_split_point]\n",
    "    test_indices = indices[calibration_split_point:]\n",
    "    \n",
    "    # Split the dataframes based on the indices\n",
    "    X_train = X.loc[train_indices]\n",
    "    Y_train = Y.loc[train_indices]\n",
    "    X_calibration = X.loc[calibration_indices]\n",
    "    Y_calibration = Y.loc[calibration_indices]\n",
    "    X_test = X.loc[test_indices]\n",
    "    Y_test = Y.loc[test_indices]\n",
    "    \n",
    "    X_train = X_train.values\n",
    "    Y_train = Y_train.values\n",
    "    X_calibration = X_calibration.values\n",
    "    Y_calibration = Y_calibration.values\n",
    "    X_test = X_test.values\n",
    "    Y_test = Y_test.values\n",
    "    \n",
    "    # Print shapes for verification\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"Y_train shape:\", Y_train.shape)\n",
    "    print(\"X_calibration shape:\", X_calibration.shape)\n",
    "    print(\"Y_calibration shape:\", Y_calibration.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"Y_test shape:\", Y_test.shape)\n",
    "\n",
    "    return X_train, Y_train, X_calibration, Y_calibration, X_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_calibration, Y_calibration, X_test, Y_test = constructTrainCalTestSplit(gi_data, gi_pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c34859-95b2-4616-a866-8ee02b51f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(Y_test, prediction, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Hamming Loss:\", hamming_loss(Y_test, prediction))\n",
    "        print(\"Accuracy Score:\", accuracy_score(Y_test, prediction))\n",
    "        print(\"F1 Score (micro):\", f1_score(Y_test, prediction, average='micro'))\n",
    "        print(\"F1 Score (macro):\", f1_score(Y_test, prediction, average='macro'))\n",
    "        print(\"Jaccard Score (average='samples'):\", jaccard_score(Y_test, prediction, average='samples'))\n",
    "        print(\"Jaccard Score (average='macro'):\", jaccard_score(Y_test, prediction, average='macro'))\n",
    "        print(\"Jaccard Score (average='micro'):\", jaccard_score(Y_test, prediction, average='micro'))\n",
    "        print(\"Jaccard Score (average=None):\", jaccard_score(Y_test, prediction, average=None))\n",
    "        print(\"Precision (macro):\", precision_score(Y_test, prediction, average='macro'))\n",
    "        print(\"Precision (micro):\", precision_score(Y_test, prediction, average='micro'))\n",
    "        print(\"Recall (micro):\", recall_score(Y_test, prediction, average='micro'))\n",
    "        print(\"Recall (macro):\", recall_score(Y_test, prediction, average='macro'))\n",
    "        print(\"Zero-One Loss (normalized):\", zero_one_loss(Y_test, prediction, normalize=True))\n",
    "\n",
    "    return [\n",
    "        hamming_loss(Y_test, prediction),\n",
    "        accuracy_score(Y_test, prediction),\n",
    "        f1_score(Y_test, prediction, average='micro'),\n",
    "        f1_score(Y_test, prediction, average='macro'),\n",
    "        jaccard_score(Y_test, prediction, average='samples'),\n",
    "        jaccard_score(Y_test, prediction, average='macro'),\n",
    "        jaccard_score(Y_test, prediction, average='micro'),\n",
    "        jaccard_score(Y_test, prediction, average=None),\n",
    "        precision_score(Y_test, prediction, average='macro'),\n",
    "        precision_score(Y_test, prediction, average='micro'),\n",
    "        recall_score(Y_test, prediction, average='micro'),\n",
    "        recall_score(Y_test, prediction, average='macro'),\n",
    "        zero_one_loss(Y_test, prediction, normalize=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e562b8-2bf2-4d9d-8daa-bfcb7beec99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0, 1, 2, 3]\n",
      "[RandomForestClassifier(), RandomForestClassifier(), RandomForestClassifier(), RandomForestClassifier()]\n"
     ]
    }
   ],
   "source": [
    "classifier = BinaryRelevance(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "# train\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# predict\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "# print classifiers info\n",
    "print(classifier.model_count_)\n",
    "print(classifier.partition_)\n",
    "print(classifier.classifiers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fbc4fc-d6fe-4239-a0e7-4af21ecafac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.18442622950819673\n",
      "Accuracy Score: 0.5573770491803278\n",
      "F1 Score (micro): 0.7457627118644068\n",
      "F1 Score (macro): 0.7126897902247975\n",
      "Jaccard Score (average='samples'): 0.3360655737704918\n",
      "Jaccard Score (average='macro'): 0.5727271513630418\n",
      "Jaccard Score (average='micro'): 0.5945945945945946\n",
      "Jaccard Score (average=None): [0.8        0.62686567 0.53846154 0.3255814 ]\n",
      "Precision (macro): 0.7087580231065469\n",
      "Precision (micro): 0.7374301675977654\n",
      "Recall (micro): 0.7542857142857143\n",
      "Recall (macro): 0.7222695046814082\n",
      "Zero-One Loss (normalized): 0.4426229508196722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18442622950819673,\n",
       " 0.5573770491803278,\n",
       " 0.7457627118644068,\n",
       " 0.7126897902247975,\n",
       " 0.3360655737704918,\n",
       " 0.5727271513630418,\n",
       " 0.5945945945945946,\n",
       " array([0.8       , 0.62686567, 0.53846154, 0.3255814 ]),\n",
       " 0.7087580231065469,\n",
       " 0.7374301675977654,\n",
       " 0.7542857142857143,\n",
       " 0.7222695046814082,\n",
       " 0.4426229508196722]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(Y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f855c6fd-691f-4066-ab7c-04b634ad412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier(), RandomForestClassifier(), RandomForestClassifier(), RandomForestClassifier()]\n"
     ]
    }
   ],
   "source": [
    "classifier = ClassifierChain(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True],\n",
    "    order=[3,2,1,0])\n",
    "\n",
    "# train\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# print classifiers info\n",
    "print(classifier.classifiers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "323bfbf2-89f3-4c59-9e41-be6747ffa489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.28688524590163933\n",
      "Accuracy Score: 0.45081967213114754\n",
      "F1 Score (micro): 0.6089385474860336\n",
      "F1 Score (macro): 0.6051531319708205\n",
      "Jaccard Score (average='samples'): 0.23975409836065573\n",
      "Jaccard Score (average='macro'): 0.4357790646853147\n",
      "Jaccard Score (average='micro'): 0.43775100401606426\n",
      "Jaccard Score (average=None): [0.38181818 0.484375   0.49230769 0.38461538]\n",
      "Precision (macro): 0.6402134646962233\n",
      "Precision (micro): 0.5956284153005464\n",
      "Recall (micro): 0.6228571428571429\n",
      "Recall (macro): 0.6496082966931329\n",
      "Zero-One Loss (normalized): 0.5491803278688525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28688524590163933,\n",
       " 0.45081967213114754,\n",
       " 0.6089385474860336,\n",
       " 0.6051531319708205,\n",
       " 0.23975409836065573,\n",
       " 0.4357790646853147,\n",
       " 0.43775100401606426,\n",
       " array([0.38181818, 0.484375  , 0.49230769, 0.38461538]),\n",
       " 0.6402134646962233,\n",
       " 0.5956284153005464,\n",
       " 0.6228571428571429,\n",
       " 0.6496082966931329,\n",
       " 0.5491803278688525]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da3602db-73ef-462d-ba3b-87fa3000b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin, ClassifierMixin\n",
    "from sklearn.utils import validation\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "class EnsembleClassifierChain(\n",
    " BaseEstimator, MetaEstimatorMixin, ClassifierMixin):\n",
    " def __init__(\n",
    "     self,\n",
    "     estimator,\n",
    "     number_of_chains=24,\n",
    "     threshold=.5,\n",
    "     max_features=1.0):\n",
    "     self.number_of_chains = number_of_chains\n",
    "     self.estimator = estimator\n",
    "     self.threshold = threshold\n",
    "     self.max_features = max_features\n",
    "     self.estimators_ = []\n",
    " def fit(self, X, y):\n",
    "     validation.check_X_y(X, y, multi_output=True)\n",
    "     y = validation.check_array(y, accept_sparse=True)\n",
    "     for i in range(self.number_of_chains):\n",
    "            # the classifier gets cloned internally in classifer chains, so\n",
    "            # no need to do that here.\n",
    "         cc = ClassifierChain(self.estimator)\n",
    "         no_samples = y.shape[0]\n",
    "            # create random subset for each chain individually\n",
    "         idx = random.sample(range(no_samples),\n",
    "                                int(no_samples * self.max_features))\n",
    "         cc.fit(X[idx, :], y[idx, :])\n",
    "         self.estimators_.append(cc)\n",
    " def predict(self, X):\n",
    "     validation.check_is_fitted(self, 'estimators_')\n",
    "     preds = np.array([cc.predict(X) for cc in self.estimators_])\n",
    "     preds = np.sum(preds, axis=0)\n",
    "     W_norm = preds.mean(axis=0)\n",
    "     out = preds / W_norm\n",
    "     return (out >= self.threshold).astype(int)\n",
    "     \n",
    "     \n",
    "classifier = EnsembleClassifierChain(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "047aa615-4666-40bd-ab99-0b9cfbf5db2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EnsembleClassifierChain' object has no attribute 'classifiers_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print classifiers info\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifiers_\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EnsembleClassifierChain' object has no attribute 'classifiers_'"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "735c7eed-a855-4389-8122-038b87a64c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.19262295081967212\n",
      "Accuracy Score: 0.5409836065573771\n",
      "F1 Score (micro): 0.7445652173913043\n",
      "F1 Score (macro): 0.7168325436107139\n",
      "Jaccard Score (average='samples'): 0.34357923497267756\n",
      "Jaccard Score (average='macro'): 0.574300221880867\n",
      "Jaccard Score (average='micro'): 0.5930735930735931\n",
      "Jaccard Score (average=None): [0.79032258 0.61428571 0.53703704 0.35555556]\n",
      "Precision (macro): 0.7539237030080538\n",
      "Precision (micro): 0.7828571428571428\n",
      "Recall (micro): 0.7098445595854922\n",
      "Recall (macro): 0.6860370950888193\n",
      "Zero-One Loss (normalized): 0.4590163934426229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a76f87-0569-42df-be6f-22c3b6bd8bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define your model architecture\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Define your model architecture\n",
    "class DeepMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepMultiLabelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Xavier initialization\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "                init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))  # Apply sigmoid activation\n",
    "        return x\n",
    "\n",
    "# Define your data loading process\n",
    "def load_data(X, Y, device, batch_size=32):\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "    joined_dataset = TensorDataset(X, Y)\n",
    "    return DataLoader(joined_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    lossTrain, lossVal = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = round(running_loss/len(train_loader.dataset), 5)\n",
    "        \n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = round(val_loss/len(val_loader.dataset), 5)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}\\nTrain Loss: {avg_train_loss}; Val Loss: {avg_val_loss}\")\n",
    "        lossTrain.append(avg_train_loss)\n",
    "        lossVal.append(avg_val_loss)\n",
    "    return lossTrain, lossVal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc9ea3-0c6e-4cdd-8d3f-8c45967b1acb",
   "metadata": {},
   "source": [
    "### Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "    # Parameters\n",
    "    input_size = len(X_train[0])\n",
    "    output_size = len(Y_train[0])\n",
    "    hidden_size = 256\n",
    "    batch_size = 32\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.000005\n",
    "\n",
    "    # Instantiate model and load data\n",
    "    model = DeepMultiLabelClassifier(input_size, hidden_size, output_size).to(device)\n",
    "    train_loader = load_data(X_train, Y_train, device, batch_size)\n",
    "    val_loader = load_data(X_calibration, Y_calibration, device, batch_size)\n",
    "    test_loader = load_data(X_test, Y_test, device, batch_size)\n",
    "    print(model)\n",
    "    # Train the model\n",
    "    hist = train_model(model, train_loader, val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a907f75f-5146-40ca-806d-be279a74d507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "DeepMultiLabelClassifier(\n",
      "  (fc1): Linear(in_features=60936, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Loss: 0.02136; Val Loss: 0.01942\n",
      "Epoch 2\n",
      "Train Loss: 0.01815; Val Loss: 0.02034\n",
      "Epoch 3\n",
      "Train Loss: 0.01694; Val Loss: 0.01905\n",
      "Epoch 4\n",
      "Train Loss: 0.01604; Val Loss: 0.02007\n",
      "Epoch 5\n",
      "Train Loss: 0.01538; Val Loss: 0.01768\n",
      "Epoch 6\n",
      "Train Loss: 0.01506; Val Loss: 0.01746\n",
      "Epoch 7\n",
      "Train Loss: 0.0143; Val Loss: 0.01739\n",
      "Epoch 8\n",
      "Train Loss: 0.01463; Val Loss: 0.01765\n",
      "Epoch 9\n",
      "Train Loss: 0.01427; Val Loss: 0.01662\n",
      "Epoch 10\n",
      "Train Loss: 0.01379; Val Loss: 0.01769\n",
      "Epoch 11\n",
      "Train Loss: 0.01299; Val Loss: 0.01614\n",
      "Epoch 12\n",
      "Train Loss: 0.01236; Val Loss: 0.01599\n",
      "Epoch 13\n",
      "Train Loss: 0.01219; Val Loss: 0.01592\n",
      "Epoch 14\n",
      "Train Loss: 0.01198; Val Loss: 0.01554\n",
      "Epoch 15\n",
      "Train Loss: 0.01178; Val Loss: 0.01817\n",
      "Epoch 16\n",
      "Train Loss: 0.01175; Val Loss: 0.01611\n",
      "Epoch 17\n",
      "Train Loss: 0.01128; Val Loss: 0.01557\n",
      "Epoch 18\n",
      "Train Loss: 0.01129; Val Loss: 0.0157\n",
      "Epoch 19\n",
      "Train Loss: 0.01088; Val Loss: 0.01638\n",
      "Epoch 20\n",
      "Train Loss: 0.01146; Val Loss: 0.01561\n",
      "Epoch 21\n",
      "Train Loss: 0.01149; Val Loss: 0.01953\n",
      "Epoch 22\n",
      "Train Loss: 0.01257; Val Loss: 0.01583\n",
      "Epoch 23\n",
      "Train Loss: 0.01111; Val Loss: 0.01529\n",
      "Epoch 24\n",
      "Train Loss: 0.01036; Val Loss: 0.01705\n",
      "Epoch 25\n",
      "Train Loss: 0.0104; Val Loss: 0.01525\n",
      "Epoch 26\n",
      "Train Loss: 0.01038; Val Loss: 0.01509\n",
      "Epoch 27\n",
      "Train Loss: 0.01038; Val Loss: 0.0163\n",
      "Epoch 28\n",
      "Train Loss: 0.01085; Val Loss: 0.01551\n",
      "Epoch 29\n",
      "Train Loss: 0.01053; Val Loss: 0.01586\n",
      "Epoch 30\n",
      "Train Loss: 0.01031; Val Loss: 0.01566\n",
      "Epoch 31\n",
      "Train Loss: 0.01039; Val Loss: 0.01533\n",
      "Epoch 32\n",
      "Train Loss: 0.00956; Val Loss: 0.0149\n",
      "Epoch 33\n",
      "Train Loss: 0.01012; Val Loss: 0.01724\n",
      "Epoch 34\n",
      "Train Loss: 0.00986; Val Loss: 0.01561\n",
      "Epoch 35\n",
      "Train Loss: 0.01004; Val Loss: 0.01519\n",
      "Epoch 36\n",
      "Train Loss: 0.00967; Val Loss: 0.01619\n",
      "Epoch 37\n",
      "Train Loss: 0.01051; Val Loss: 0.0149\n",
      "Epoch 38\n",
      "Train Loss: 0.00947; Val Loss: 0.01541\n",
      "Epoch 39\n",
      "Train Loss: 0.00947; Val Loss: 0.0159\n",
      "Epoch 40\n",
      "Train Loss: 0.00945; Val Loss: 0.01554\n",
      "Epoch 41\n",
      "Train Loss: 0.00948; Val Loss: 0.01749\n",
      "Epoch 42\n",
      "Train Loss: 0.00925; Val Loss: 0.01546\n",
      "Epoch 43\n",
      "Train Loss: 0.00912; Val Loss: 0.01558\n",
      "Epoch 44\n",
      "Train Loss: 0.00919; Val Loss: 0.01477\n",
      "Epoch 45\n",
      "Train Loss: 0.00914; Val Loss: 0.01591\n",
      "Epoch 46\n",
      "Train Loss: 0.00899; Val Loss: 0.01575\n",
      "Epoch 47\n",
      "Train Loss: 0.01006; Val Loss: 0.01489\n",
      "Epoch 48\n",
      "Train Loss: 0.00901; Val Loss: 0.01533\n",
      "Epoch 49\n",
      "Train Loss: 0.0092; Val Loss: 0.01579\n",
      "Epoch 50\n",
      "Train Loss: 0.00865; Val Loss: 0.01688\n",
      "Epoch 51\n",
      "Train Loss: 0.00953; Val Loss: 0.0175\n",
      "Epoch 52\n",
      "Train Loss: 0.00951; Val Loss: 0.01534\n",
      "Epoch 53\n",
      "Train Loss: 0.00906; Val Loss: 0.01616\n",
      "Epoch 54\n",
      "Train Loss: 0.0087; Val Loss: 0.01543\n",
      "Epoch 55\n",
      "Train Loss: 0.00835; Val Loss: 0.01729\n",
      "Epoch 56\n",
      "Train Loss: 0.00911; Val Loss: 0.01559\n",
      "Epoch 57\n",
      "Train Loss: 0.00868; Val Loss: 0.0155\n",
      "Epoch 58\n",
      "Train Loss: 0.00848; Val Loss: 0.01635\n",
      "Epoch 59\n",
      "Train Loss: 0.00851; Val Loss: 0.01506\n",
      "Epoch 60\n",
      "Train Loss: 0.00895; Val Loss: 0.01554\n",
      "Epoch 61\n",
      "Train Loss: 0.00823; Val Loss: 0.0164\n",
      "Epoch 62\n",
      "Train Loss: 0.00857; Val Loss: 0.01537\n",
      "Epoch 63\n",
      "Train Loss: 0.00837; Val Loss: 0.01728\n",
      "Epoch 64\n",
      "Train Loss: 0.00881; Val Loss: 0.01791\n",
      "Epoch 65\n",
      "Train Loss: 0.0084; Val Loss: 0.01657\n",
      "Epoch 66\n",
      "Train Loss: 0.00839; Val Loss: 0.01507\n",
      "Epoch 67\n",
      "Train Loss: 0.00834; Val Loss: 0.01903\n",
      "Epoch 68\n",
      "Train Loss: 0.0082; Val Loss: 0.01574\n",
      "Epoch 69\n",
      "Train Loss: 0.00819; Val Loss: 0.01629\n",
      "Epoch 70\n",
      "Train Loss: 0.00802; Val Loss: 0.0166\n",
      "Epoch 71\n",
      "Train Loss: 0.0079; Val Loss: 0.01554\n",
      "Epoch 72\n",
      "Train Loss: 0.0078; Val Loss: 0.01593\n",
      "Epoch 73\n",
      "Train Loss: 0.00814; Val Loss: 0.01589\n",
      "Epoch 74\n",
      "Train Loss: 0.00813; Val Loss: 0.01596\n",
      "Epoch 75\n",
      "Train Loss: 0.00815; Val Loss: 0.01938\n",
      "Epoch 76\n",
      "Train Loss: 0.00844; Val Loss: 0.01577\n",
      "Epoch 77\n",
      "Train Loss: 0.00826; Val Loss: 0.016\n",
      "Epoch 78\n",
      "Train Loss: 0.0079; Val Loss: 0.01683\n",
      "Epoch 79\n",
      "Train Loss: 0.00845; Val Loss: 0.01546\n",
      "Epoch 80\n",
      "Train Loss: 0.00857; Val Loss: 0.01579\n",
      "Epoch 81\n",
      "Train Loss: 0.00785; Val Loss: 0.0171\n",
      "Epoch 82\n",
      "Train Loss: 0.00838; Val Loss: 0.01628\n",
      "Epoch 83\n",
      "Train Loss: 0.00737; Val Loss: 0.01631\n",
      "Epoch 84\n",
      "Train Loss: 0.00806; Val Loss: 0.01725\n",
      "Epoch 85\n",
      "Train Loss: 0.00858; Val Loss: 0.01595\n",
      "Epoch 86\n",
      "Train Loss: 0.00838; Val Loss: 0.0165\n",
      "Epoch 87\n",
      "Train Loss: 0.00788; Val Loss: 0.01627\n",
      "Epoch 88\n",
      "Train Loss: 0.00848; Val Loss: 0.01539\n",
      "Epoch 89\n",
      "Train Loss: 0.00821; Val Loss: 0.01589\n",
      "Epoch 90\n",
      "Train Loss: 0.00783; Val Loss: 0.01563\n",
      "Epoch 91\n",
      "Train Loss: 0.00758; Val Loss: 0.01629\n",
      "Epoch 92\n",
      "Train Loss: 0.00762; Val Loss: 0.01657\n",
      "Epoch 93\n",
      "Train Loss: 0.0079; Val Loss: 0.01582\n",
      "Epoch 94\n",
      "Train Loss: 0.00739; Val Loss: 0.01562\n",
      "Epoch 95\n",
      "Train Loss: 0.00723; Val Loss: 0.01571\n",
      "Epoch 96\n",
      "Train Loss: 0.00722; Val Loss: 0.01602\n",
      "Epoch 97\n",
      "Train Loss: 0.00721; Val Loss: 0.01731\n",
      "Epoch 98\n",
      "Train Loss: 0.00717; Val Loss: 0.01713\n",
      "Epoch 99\n",
      "Train Loss: 0.00775; Val Loss: 0.0164\n",
      "Epoch 100\n",
      "Train Loss: 0.00721; Val Loss: 0.01564\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "    # Parameters\n",
    "    input_size = len(X_train[0])\n",
    "    output_size = len(Y_train[0])\n",
    "    hidden_size = 256\n",
    "    batch_size = 32\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.000005\n",
    "\n",
    "    # Instantiate model and load data\n",
    "    model = DeepMultiLabelClassifier(input_size, hidden_size, output_size).to(device)\n",
    "    train_loader = load_data(X_train, Y_train, device, batch_size)\n",
    "    val_loader = load_data(X_calibration, Y_calibration, device, batch_size)\n",
    "    test_loader = load_data(X_test, Y_test, device, batch_size)\n",
    "    print(model)\n",
    "    # Train the model\n",
    "    hist = train_model(model, train_loader, val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac45d43b-e454-4ca6-89b0-2b608480e53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.1885245901639344\n",
      "Accuracy Score: 0.5983606557377049\n",
      "F1 Score (micro): 0.7513513513513513\n",
      "F1 Score (macro): 0.7340785200043647\n",
      "Jaccard Score (average='samples'): 0.3394808743169399\n",
      "Jaccard Score (average='macro'): 0.5912243494210707\n",
      "Jaccard Score (average='micro'): 0.6017316017316018\n",
      "Jaccard Score (average=None): [0.7704918  0.63076923 0.56363636 0.4       ]\n",
      "Precision (macro): 0.6960091416819849\n",
      "Precision (micro): 0.7128205128205128\n",
      "Recall (micro): 0.7942857142857143\n",
      "Recall (macro): 0.7787563319957881\n",
      "Zero-One Loss (normalized): 0.4016393442622951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1885245901639344,\n",
       " 0.5983606557377049,\n",
       " 0.7513513513513513,\n",
       " 0.7340785200043647,\n",
       " 0.3394808743169399,\n",
       " 0.5912243494210707,\n",
       " 0.6017316017316018,\n",
       " array([0.7704918 , 0.63076923, 0.56363636, 0.4       ]),\n",
       " 0.6960091416819849,\n",
       " 0.7128205128205128,\n",
       " 0.7942857142857143,\n",
       " 0.7787563319957881,\n",
       " 0.4016393442622951]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_raw = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "pred = torch.round(pred_raw)\n",
    "eval_metrics(Y_test, pred.to(\"cpu\").detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298141cd-113f-415a-bd17-e306126a8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_raw = model(torch.tensor(X_calibration, dtype=torch.float32).to(device))\n",
    "pred = torch.round(pred_raw)\n",
    "eval_metrics(Y_calibration, pred.to(\"cpu\").detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9912090a-3f4b-41a7-9639-63c53ab60de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "DeepMultiLabelClassifier(\n",
      "  (fc1): Linear(in_features=60936, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Loss: 0.02101; Val Loss: 0.02009\n",
      "Epoch 2\n",
      "Train Loss: 0.01834; Val Loss: 0.01883\n",
      "Epoch 3\n",
      "Train Loss: 0.0174; Val Loss: 0.01922\n",
      "Epoch 4\n",
      "Train Loss: 0.01749; Val Loss: 0.01823\n",
      "Epoch 5\n",
      "Train Loss: 0.01596; Val Loss: 0.01762\n",
      "Epoch 6\n",
      "Train Loss: 0.01528; Val Loss: 0.01893\n",
      "Epoch 7\n",
      "Train Loss: 0.01492; Val Loss: 0.01751\n",
      "Epoch 8\n",
      "Train Loss: 0.014; Val Loss: 0.01756\n",
      "Epoch 9\n",
      "Train Loss: 0.0138; Val Loss: 0.01651\n",
      "Epoch 10\n",
      "Train Loss: 0.01299; Val Loss: 0.01643\n",
      "Epoch 11\n",
      "Train Loss: 0.01353; Val Loss: 0.01653\n",
      "Epoch 12\n",
      "Train Loss: 0.01279; Val Loss: 0.01625\n",
      "Epoch 13\n",
      "Train Loss: 0.01233; Val Loss: 0.01621\n",
      "Epoch 14\n",
      "Train Loss: 0.01204; Val Loss: 0.0162\n",
      "Epoch 15\n",
      "Train Loss: 0.01202; Val Loss: 0.01564\n",
      "Epoch 16\n",
      "Train Loss: 0.01163; Val Loss: 0.01701\n",
      "Epoch 17\n",
      "Train Loss: 0.01109; Val Loss: 0.01564\n",
      "Epoch 18\n",
      "Train Loss: 0.01114; Val Loss: 0.01504\n",
      "Epoch 19\n",
      "Train Loss: 0.01098; Val Loss: 0.01525\n",
      "Epoch 20\n",
      "Train Loss: 0.01082; Val Loss: 0.01652\n",
      "Epoch 21\n",
      "Train Loss: 0.01083; Val Loss: 0.01616\n",
      "Epoch 22\n",
      "Train Loss: 0.01119; Val Loss: 0.01652\n",
      "Epoch 23\n",
      "Train Loss: 0.01134; Val Loss: 0.01541\n",
      "Epoch 24\n",
      "Train Loss: 0.01047; Val Loss: 0.0158\n",
      "Epoch 25\n",
      "Train Loss: 0.01081; Val Loss: 0.01535\n",
      "Epoch 26\n",
      "Train Loss: 0.01102; Val Loss: 0.01578\n",
      "Epoch 27\n",
      "Train Loss: 0.01052; Val Loss: 0.0162\n",
      "Epoch 28\n",
      "Train Loss: 0.00977; Val Loss: 0.01524\n",
      "Epoch 29\n",
      "Train Loss: 0.01021; Val Loss: 0.01535\n",
      "Epoch 30\n",
      "Train Loss: 0.01028; Val Loss: 0.0149\n",
      "Epoch 31\n",
      "Train Loss: 0.0096; Val Loss: 0.0145\n",
      "Epoch 32\n",
      "Train Loss: 0.01014; Val Loss: 0.01504\n",
      "Epoch 33\n",
      "Train Loss: 0.00956; Val Loss: 0.01606\n",
      "Epoch 34\n",
      "Train Loss: 0.01012; Val Loss: 0.01621\n",
      "Epoch 35\n",
      "Train Loss: 0.01126; Val Loss: 0.01658\n",
      "Epoch 36\n",
      "Train Loss: 0.00965; Val Loss: 0.01532\n",
      "Epoch 37\n",
      "Train Loss: 0.00937; Val Loss: 0.01539\n",
      "Epoch 38\n",
      "Train Loss: 0.00914; Val Loss: 0.01493\n",
      "Epoch 39\n",
      "Train Loss: 0.00874; Val Loss: 0.01484\n",
      "Epoch 40\n",
      "Train Loss: 0.00924; Val Loss: 0.0167\n",
      "Epoch 41\n",
      "Train Loss: 0.00918; Val Loss: 0.01513\n",
      "Epoch 42\n",
      "Train Loss: 0.0089; Val Loss: 0.01518\n",
      "Epoch 43\n",
      "Train Loss: 0.00935; Val Loss: 0.01809\n",
      "Epoch 44\n",
      "Train Loss: 0.01035; Val Loss: 0.01642\n",
      "Epoch 45\n",
      "Train Loss: 0.00941; Val Loss: 0.01711\n",
      "Epoch 46\n",
      "Train Loss: 0.009; Val Loss: 0.01527\n",
      "Epoch 47\n",
      "Train Loss: 0.00868; Val Loss: 0.01604\n",
      "Epoch 48\n",
      "Train Loss: 0.00857; Val Loss: 0.01535\n",
      "Epoch 49\n",
      "Train Loss: 0.00885; Val Loss: 0.01603\n",
      "Epoch 50\n",
      "Train Loss: 0.0089; Val Loss: 0.01528\n",
      "Epoch 51\n",
      "Train Loss: 0.00869; Val Loss: 0.01547\n",
      "Epoch 52\n",
      "Train Loss: 0.00968; Val Loss: 0.01634\n",
      "Epoch 53\n",
      "Train Loss: 0.00889; Val Loss: 0.01504\n",
      "Epoch 54\n",
      "Train Loss: 0.00873; Val Loss: 0.01625\n",
      "Epoch 55\n",
      "Train Loss: 0.01012; Val Loss: 0.01588\n",
      "Epoch 56\n",
      "Train Loss: 0.01012; Val Loss: 0.01606\n",
      "Epoch 57\n",
      "Train Loss: 0.00899; Val Loss: 0.01687\n",
      "Epoch 58\n",
      "Train Loss: 0.00865; Val Loss: 0.01605\n",
      "Epoch 59\n",
      "Train Loss: 0.00851; Val Loss: 0.01651\n",
      "Epoch 60\n",
      "Train Loss: 0.00861; Val Loss: 0.01541\n",
      "Epoch 61\n",
      "Train Loss: 0.00837; Val Loss: 0.01523\n",
      "Epoch 62\n",
      "Train Loss: 0.00791; Val Loss: 0.01618\n",
      "Epoch 63\n",
      "Train Loss: 0.00845; Val Loss: 0.01643\n",
      "Epoch 64\n",
      "Train Loss: 0.0085; Val Loss: 0.01683\n",
      "Epoch 65\n",
      "Train Loss: 0.00922; Val Loss: 0.0178\n",
      "Epoch 66\n",
      "Train Loss: 0.00947; Val Loss: 0.01678\n",
      "Epoch 67\n",
      "Train Loss: 0.00893; Val Loss: 0.01624\n",
      "Epoch 68\n",
      "Train Loss: 0.00891; Val Loss: 0.01717\n",
      "Epoch 69\n",
      "Train Loss: 0.00864; Val Loss: 0.01553\n",
      "Epoch 70\n",
      "Train Loss: 0.00814; Val Loss: 0.01753\n",
      "Epoch 71\n",
      "Train Loss: 0.00826; Val Loss: 0.01588\n",
      "Epoch 72\n",
      "Train Loss: 0.00918; Val Loss: 0.01764\n",
      "Epoch 73\n",
      "Train Loss: 0.00873; Val Loss: 0.01593\n",
      "Epoch 74\n",
      "Train Loss: 0.00794; Val Loss: 0.0156\n",
      "Epoch 75\n",
      "Train Loss: 0.00819; Val Loss: 0.01722\n",
      "Epoch 76\n",
      "Train Loss: 0.00764; Val Loss: 0.01536\n",
      "Epoch 77\n",
      "Train Loss: 0.00739; Val Loss: 0.01713\n",
      "Epoch 78\n",
      "Train Loss: 0.00769; Val Loss: 0.0176\n",
      "Epoch 79\n",
      "Train Loss: 0.00778; Val Loss: 0.01546\n",
      "Epoch 80\n",
      "Train Loss: 0.00775; Val Loss: 0.01603\n",
      "Epoch 81\n",
      "Train Loss: 0.00869; Val Loss: 0.01618\n",
      "Epoch 82\n",
      "Train Loss: 0.0078; Val Loss: 0.01867\n",
      "Epoch 83\n",
      "Train Loss: 0.00909; Val Loss: 0.0163\n",
      "Epoch 84\n",
      "Train Loss: 0.00792; Val Loss: 0.01925\n",
      "Epoch 85\n",
      "Train Loss: 0.00828; Val Loss: 0.01681\n",
      "Epoch 86\n",
      "Train Loss: 0.00765; Val Loss: 0.01611\n",
      "Epoch 87\n",
      "Train Loss: 0.00745; Val Loss: 0.01648\n",
      "Epoch 88\n",
      "Train Loss: 0.00766; Val Loss: 0.01603\n",
      "Epoch 89\n",
      "Train Loss: 0.00791; Val Loss: 0.01554\n",
      "Epoch 90\n",
      "Train Loss: 0.00752; Val Loss: 0.01528\n",
      "Epoch 91\n",
      "Train Loss: 0.00731; Val Loss: 0.01698\n",
      "Epoch 92\n",
      "Train Loss: 0.00779; Val Loss: 0.01665\n",
      "Epoch 93\n",
      "Train Loss: 0.00731; Val Loss: 0.01548\n",
      "Epoch 94\n",
      "Train Loss: 0.00793; Val Loss: 0.01564\n",
      "Epoch 95\n",
      "Train Loss: 0.00844; Val Loss: 0.01666\n",
      "Epoch 96\n",
      "Train Loss: 0.00771; Val Loss: 0.01547\n",
      "Epoch 97\n",
      "Train Loss: 0.00709; Val Loss: 0.01591\n",
      "Epoch 98\n",
      "Train Loss: 0.00731; Val Loss: 0.01577\n",
      "Epoch 99\n",
      "Train Loss: 0.0073; Val Loss: 0.01714\n",
      "Epoch 100\n",
      "Train Loss: 0.00742; Val Loss: 0.01607\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "    # Parameters\n",
    "    input_size = len(X_train[0])\n",
    "    output_size = len(Y_train[0])\n",
    "    hidden_size = 256\n",
    "    batch_size = 32\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.000005\n",
    "\n",
    "    # Instantiate model and load data\n",
    "    model = DeepMultiLabelClassifier(input_size, hidden_size, output_size).to(device)\n",
    "    train_loader = load_data(X_train, Y_train, device, batch_size)\n",
    "    val_loader = load_data(X_calibration, Y_calibration, device, batch_size)\n",
    "    test_loader = load_data(X_test, Y_test, device, batch_size)\n",
    "    print(model)\n",
    "    # Train the model\n",
    "    hist = train_model(model, train_loader, val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357cbada-91e1-4dc6-a23b-213f0e1d31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model-baseline-mlp-100ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "431425fe-7abc-4076-93b0-c8f99d9f8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.1921487603305785\n",
      "Accuracy Score: 0.5702479338842975\n",
      "F1 Score (micro): 0.7465940054495913\n",
      "F1 Score (macro): 0.7240520708775036\n",
      "Jaccard Score (average='samples'): 0.3546831955922865\n",
      "Jaccard Score (average='macro'): 0.5826664297559819\n",
      "Jaccard Score (average='micro'): 0.5956521739130435\n",
      "Jaccard Score (average=None): [0.79365079 0.59701493 0.58       0.36      ]\n",
      "Precision (macro): 0.7044595490716181\n",
      "Precision (micro): 0.7248677248677249\n",
      "Recall (micro): 0.7696629213483146\n",
      "Recall (macro): 0.7501607587814484\n",
      "Zero-One Loss (normalized): 0.4297520661157025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1921487603305785,\n",
       " 0.5702479338842975,\n",
       " 0.7465940054495913,\n",
       " 0.7240520708775036,\n",
       " 0.3546831955922865,\n",
       " 0.5826664297559819,\n",
       " 0.5956521739130435,\n",
       " array([0.79365079, 0.59701493, 0.58      , 0.36      ]),\n",
       " 0.7044595490716181,\n",
       " 0.7248677248677249,\n",
       " 0.7696629213483146,\n",
       " 0.7501607587814484,\n",
       " 0.4297520661157025]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_raw = model(torch.tensor(X_calibration, dtype=torch.float32).to(device))\n",
    "pred = torch.round(pred_raw)\n",
    "eval_metrics(Y_calibration, pred.to(\"cpu\").detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81e1a6e8-2fd4-4f80-b5c7-d633375db75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.1864754098360656\n",
      "Accuracy Score: 0.5901639344262295\n",
      "F1 Score (micro): 0.7493112947658402\n",
      "F1 Score (macro): 0.7303400969587208\n",
      "Jaccard Score (average='samples'): 0.328551912568306\n",
      "Jaccard Score (average='macro'): 0.5878193648203291\n",
      "Jaccard Score (average='micro'): 0.5991189427312775\n",
      "Jaccard Score (average=None): [0.76190476 0.6557377  0.54901961 0.38461538]\n",
      "Precision (macro): 0.7083333333333333\n",
      "Precision (micro): 0.723404255319149\n",
      "Recall (micro): 0.7771428571428571\n",
      "Recall (macro): 0.7597105424172572\n",
      "Zero-One Loss (normalized): 0.4098360655737705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1864754098360656,\n",
       " 0.5901639344262295,\n",
       " 0.7493112947658402,\n",
       " 0.7303400969587208,\n",
       " 0.328551912568306,\n",
       " 0.5878193648203291,\n",
       " 0.5991189427312775,\n",
       " array([0.76190476, 0.6557377 , 0.54901961, 0.38461538]),\n",
       " 0.7083333333333333,\n",
       " 0.723404255319149,\n",
       " 0.7771428571428571,\n",
       " 0.7597105424172572,\n",
       " 0.4098360655737705]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_raw = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "pred = torch.round(pred_raw)\n",
    "eval_metrics(Y_test, pred.to(\"cpu\").detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "603596ee-d8af-49f7-9618-a82717797699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred cal <class 'numpy.ndarray'> (121, 4)\n",
      "true cal <class 'numpy.ndarray'> (121, 4)\n",
      "pred val <class 'numpy.ndarray'> (122, 4)\n",
      "true val <class 'numpy.ndarray'> (122, 4)\n"
     ]
    }
   ],
   "source": [
    "pred_raw = model(torch.tensor(X_calibration, dtype=torch.float32).to(device))\n",
    "cal_sgmd = np.round(pred_raw.detach().cpu().numpy(), decimals=5)\n",
    "cal_labels = Y_calibration\n",
    "pred_raw = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "val_sgmd = np.round(pred_raw.detach().cpu().numpy(), decimals=5)\n",
    "val_labels = Y_test\n",
    "print(\"pred cal\", type(cal_sgmd), cal_sgmd.shape)\n",
    "print(\"true cal\", type(cal_labels), cal_labels.shape)\n",
    "print(\"pred val\", type(val_sgmd), val_sgmd.shape)\n",
    "print(\"true val\", type(val_labels), val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8057e6a7-e0c9-4386-a8ff-04d3e210a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnr_custom(preds, labels):\n",
    "    # FNR for class i: FN / FN + TP\n",
    "    classes = {}\n",
    "    for j in range(len(preds[0])): # iterate over classes\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        for i in range(len(preds)): # iterate over exs\n",
    "            if labels[i,j] == 1:\n",
    "                if preds[i,j] != 1: FN += 1\n",
    "                else: TP += 1\n",
    "        if FN + TP > 0: # if label never appears, dont worry about it\n",
    "            classes[j] = {\"FN\": FN, \"TP\": TP, \"cnt\": FN + TP}\n",
    "    # print(classes)\n",
    "    total = sum(classes[lbl][\"cnt\"] for lbl in classes)\n",
    "    res = 0\n",
    "    for lbl in classes: \n",
    "        res += (classes[lbl][\"FN\"] / total)\n",
    "    return res\n",
    "    \n",
    "# def false_negative_rate(prediction_set, gt_labels):\n",
    "#     return 1-((prediction_set * gt_labels).sum(axis=1)/gt_labels.sum(axis=1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acf0d3ff-ad43-4846-9d70-a2e44c68ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029940000735976892"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import brentq\n",
    "alpha = 0.05\n",
    "n = len(X_calibration)\n",
    "# Run the conformal risk control procedure\n",
    "def lamhat_threshold(lam): return fnr_custom(cal_sgmd>=lam, cal_labels) - ((n+1)/n*alpha - 1/(n+1))\n",
    "lamhat = brentq(lamhat_threshold, 0.001, 0.999)\n",
    "# prediction_sets = val_sgmd >= lamhat\n",
    "\n",
    "lamhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ae65145-1a24-40c0-ac97-e3b11c732a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_sets = val_sgmd >= lamhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db931045-7e44-437b-9418-da936ac3e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical FNR is: 0.03428571428571429 and the threshold value is: 0.029940000735976892\n"
     ]
    }
   ],
   "source": [
    "# Calculate empirical FNR\n",
    "print(f\"The empirical FNR is: {fnr_custom(prediction_sets, val_labels)} and the threshold value is: {lamhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4f10765-a24a-4305-930c-c44be8d63c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.42418032786885246\n",
      "Accuracy Score: 0.2540983606557377\n",
      "F1 Score (micro): 0.620183486238532\n",
      "F1 Score (macro): 0.614226854443918\n",
      "Jaccard Score (average='samples'): 0.3620218579234972\n",
      "Jaccard Score (average='macro'): 0.4449987512487512\n",
      "Jaccard Score (average='micro'): 0.449468085106383\n",
      "Jaccard Score (average=None): [0.5        0.49038462 0.4        0.38961039]\n",
      "Precision (macro): 0.4523264560498603\n",
      "Precision (micro): 0.45675675675675675\n",
      "Recall (micro): 0.9657142857142857\n",
      "Recall (macro): 0.963628820541279\n",
      "Zero-One Loss (normalized): 0.7459016393442623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42418032786885246,\n",
       " 0.2540983606557377,\n",
       " 0.620183486238532,\n",
       " 0.614226854443918,\n",
       " 0.3620218579234972,\n",
       " 0.4449987512487512,\n",
       " 0.449468085106383,\n",
       " array([0.5       , 0.49038462, 0.4       , 0.38961039]),\n",
       " 0.4523264560498603,\n",
       " 0.45675675675675675,\n",
       " 0.9657142857142857,\n",
       " 0.963628820541279,\n",
       " 0.7459016393442623]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(Y_test, prediction_sets.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "221057c3-602f-48ed-af4b-c277e043e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical FNR without thresholding is: 0.22285714285714286 and the threshold value is: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate empirical FNR\n",
    "print(f\"The empirical FNR without thresholding is: {fnr_custom(np.round(val_sgmd), val_labels)} and the threshold value is: 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7b80781b-e335-4944-b60d-18de324e739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.1864754098360656\n",
      "Accuracy Score: 0.5573770491803278\n",
      "F1 Score (micro): 0.7407407407407407\n",
      "F1 Score (macro): 0.7115786469963089\n",
      "Jaccard Score (average='samples'): 0.33265027322404367\n",
      "Jaccard Score (average='macro'): 0.5682736415167643\n",
      "Jaccard Score (average='micro'): 0.5882352941176471\n",
      "Jaccard Score (average=None): [0.79661017 0.57746479 0.54901961 0.35      ]\n",
      "Precision (macro): 0.7214560862865947\n",
      "Precision (micro): 0.7386363636363636\n",
      "Recall (micro): 0.7428571428571429\n",
      "Recall (macro): 0.7126505627650189\n",
      "Zero-One Loss (normalized): 0.4426229508196722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/home/bys2107/.conda/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_metrics(Y_test, prediction_sets.to(\"cpu\").detach().numpy().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3b4c905-5e24-4a16-953f-55839fdfd42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_sgmd\n",
    "val_sgmd\n",
    "cal_labels\n",
    "val_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b71a258-6a86-4049-b68e-57501059e1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.      , 0.001001, 0.002002], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8ed5a44-77f9-4370-9150-d506f5039789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0280, 0.0024, 0.0025, 0.0017])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "sigmoids = torch.tensor(cal_sgmd[i], dtype=torch.float32)\n",
    "gts = torch.tensor(cal_labels[i], dtype=torch.float32)\n",
    "print(sigmoids)\n",
    "print(gts)\n",
    "sigmoids == gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "201a0a8e-44fb-46f0-a1ac-90b1aba1e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "n = len(cal_labels)\n",
    "alpha = 0.1\n",
    "delta = 0.1\n",
    "lambdas = torch.linspace(0,1,N) # N can be taken to infinity without penalty\n",
    "losses = torch.zeros((n,N)) # loss for example i with parameter lambdas[j]\n",
    "for i in range(n): # In reality we parallelize these loops massively\n",
    "    sigmoids = torch.tensor(cal_sgmd[i], dtype=torch.float32)\n",
    "    gts = torch.tensor(cal_labels[i], dtype=torch.float32)\n",
    "    for j in range(N):\n",
    "        T = sigmoids > lambdas[j] # This is the prediction set\n",
    "        # print(T)\n",
    "        set_size = T.float().sum()\n",
    "        # print(set_size)\n",
    "        if set_size != 0:\n",
    "            losses[i,j] = 1 - (T == gts).float().sum()/set_size\n",
    "risk = losses.mean(dim=0)\n",
    "pvals = torch.exp(-2*n*(torch.relu(alpha-risk)**2)) # Or the HB p-value\n",
    "# Fixed-sequence test starting at lambdas[-1] and ending at lambdas[0]\n",
    "below_delta = (pvals <= delta).float()\n",
    "valid = torch.tensor([(below_delta[j:].mean() == 1) for j in range(N)])\n",
    "lambda_hat = lambdas[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3659a6df-cef9-4f64-8bfa-0478eae569d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0961, 0.0971, 0.0981, 0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041,\n",
       "        0.1051, 0.1061, 0.1071, 0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131,\n",
       "        0.1141, 0.1151, 0.1161, 0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221,\n",
       "        0.1231, 0.1241, 0.1251, 0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311,\n",
       "        0.1321, 0.1331, 0.1341, 0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401,\n",
       "        0.1411, 0.1421, 0.1431, 0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491,\n",
       "        0.1502, 0.1512, 0.1522, 0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582,\n",
       "        0.1592, 0.1602, 0.1612, 0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672,\n",
       "        0.1682, 0.1692, 0.1702, 0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762,\n",
       "        0.1772, 0.1782, 0.1792, 0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852,\n",
       "        0.1862, 0.1872, 0.1882, 0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942,\n",
       "        0.1952, 0.1962, 0.1972, 0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032,\n",
       "        0.2042, 0.2052, 0.2062, 0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122,\n",
       "        0.2132, 0.2142, 0.2152, 0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212,\n",
       "        0.2222, 0.2232, 0.2242, 0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302,\n",
       "        0.2312, 0.2322, 0.2332, 0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392,\n",
       "        0.2402, 0.2412, 0.2422, 0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482,\n",
       "        0.2492, 0.2503, 0.2513, 0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573,\n",
       "        0.2583, 0.2593, 0.2603, 0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663,\n",
       "        0.2673, 0.2683, 0.2693, 0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753,\n",
       "        0.2763, 0.2773, 0.2783, 0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843,\n",
       "        0.2853, 0.2863, 0.2873, 0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933,\n",
       "        0.2943, 0.2953, 0.2963, 0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023,\n",
       "        0.3033, 0.3043, 0.3053, 0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113,\n",
       "        0.3123, 0.3133, 0.3143, 0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203,\n",
       "        0.3213, 0.3223, 0.3233, 0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293,\n",
       "        0.3303, 0.3313, 0.3323, 0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383,\n",
       "        0.3393, 0.3403, 0.3413, 0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473,\n",
       "        0.3483, 0.3493, 0.3504, 0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564,\n",
       "        0.3574, 0.3584, 0.3594, 0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654,\n",
       "        0.3664, 0.3674, 0.3684, 0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744,\n",
       "        0.3754, 0.3764, 0.3774, 0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834,\n",
       "        0.3844, 0.3854, 0.3864, 0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924,\n",
       "        0.3934, 0.3944, 0.3954, 0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014,\n",
       "        0.4024, 0.4034, 0.4044, 0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104,\n",
       "        0.4114, 0.4124, 0.4134, 0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194,\n",
       "        0.4204, 0.4214, 0.4224, 0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284,\n",
       "        0.4294, 0.4304, 0.4314, 0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374,\n",
       "        0.4384, 0.4394, 0.4404, 0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464,\n",
       "        0.4474, 0.4484, 0.4494, 0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555,\n",
       "        0.4565, 0.4575, 0.4585, 0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645,\n",
       "        0.4655, 0.4665, 0.4675, 0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735,\n",
       "        0.4745, 0.4755, 0.4765, 0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825,\n",
       "        0.4835, 0.4845, 0.4855, 0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915,\n",
       "        0.4925, 0.4935, 0.4945, 0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005,\n",
       "        0.5015, 0.5025, 0.5035, 0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095,\n",
       "        0.5105, 0.5115, 0.5125, 0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185,\n",
       "        0.5195, 0.5205, 0.5215, 0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275,\n",
       "        0.5285, 0.5295, 0.5305, 0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365,\n",
       "        0.5375, 0.5385, 0.5395, 0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455,\n",
       "        0.5465, 0.5475, 0.5485, 0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546,\n",
       "        0.5556, 0.5566, 0.5576, 0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636,\n",
       "        0.5646, 0.5656, 0.5666, 0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726,\n",
       "        0.5736, 0.5746, 0.5756, 0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816,\n",
       "        0.5826, 0.5836, 0.5846, 0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906,\n",
       "        0.5916, 0.5926, 0.5936, 0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996,\n",
       "        0.6006, 0.6016, 0.6026, 0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086,\n",
       "        0.6096, 0.6106, 0.6116, 0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176,\n",
       "        0.6186, 0.6196, 0.6206, 0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266,\n",
       "        0.6276, 0.6286, 0.6296, 0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356,\n",
       "        0.6366, 0.6376, 0.6386, 0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446,\n",
       "        0.6456, 0.6466, 0.6476, 0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537,\n",
       "        0.6547, 0.6557, 0.6567, 0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627,\n",
       "        0.6637, 0.6647, 0.6657, 0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717,\n",
       "        0.6727, 0.6737, 0.6747, 0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807,\n",
       "        0.6817, 0.6827, 0.6837, 0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897,\n",
       "        0.6907, 0.6917, 0.6927, 0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987,\n",
       "        0.6997, 0.7007, 0.7017, 0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077,\n",
       "        0.7087, 0.7097, 0.7107, 0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167,\n",
       "        0.7177, 0.7187, 0.7197, 0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257,\n",
       "        0.7267, 0.7277, 0.7287, 0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347,\n",
       "        0.7357, 0.7367, 0.7377, 0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437,\n",
       "        0.7447, 0.7457, 0.7467, 0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528,\n",
       "        0.7538, 0.7548, 0.7558, 0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618,\n",
       "        0.7628, 0.7638, 0.7648, 0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708,\n",
       "        0.7718, 0.7728, 0.7738, 0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798,\n",
       "        0.7808, 0.7818, 0.7828, 0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888,\n",
       "        0.7898, 0.7908, 0.7918, 0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978,\n",
       "        0.7988, 0.7998, 0.8008, 0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068,\n",
       "        0.8078, 0.8088, 0.8098, 0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158,\n",
       "        0.8168, 0.8178, 0.8188, 0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248,\n",
       "        0.8258, 0.8268, 0.8278, 0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338,\n",
       "        0.8348, 0.8358, 0.8368, 0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428,\n",
       "        0.8438, 0.8448, 0.8458, 0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519,\n",
       "        0.8529, 0.8539, 0.8549, 0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609,\n",
       "        0.8619, 0.8629, 0.8639, 0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699,\n",
       "        0.8709, 0.8719, 0.8729, 0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789,\n",
       "        0.8799, 0.8809, 0.8819, 0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879,\n",
       "        0.8889, 0.8899, 0.8909, 0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969,\n",
       "        0.8979, 0.8989, 0.8999, 0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059,\n",
       "        0.9069, 0.9079, 0.9089, 0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149,\n",
       "        0.9159, 0.9169, 0.9179, 0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239,\n",
       "        0.9249, 0.9259, 0.9269, 0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329,\n",
       "        0.9339, 0.9349, 0.9359, 0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419,\n",
       "        0.9429, 0.9439, 0.9449, 0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510,\n",
       "        0.9520, 0.9530, 0.9540, 0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600,\n",
       "        0.9610, 0.9620, 0.9630, 0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690,\n",
       "        0.9700, 0.9710, 0.9720, 0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780,\n",
       "        0.9790, 0.9800, 0.9810, 0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870,\n",
       "        0.9880, 0.9890, 0.9900, 0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960,\n",
       "        0.9970, 0.9980, 0.9990, 1.0000])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e2baf-f209-4b32-b944-21fb15ccffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
